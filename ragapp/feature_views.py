# ragapp/feature_views.py
from __future__ import annotations

import os
import csv
import mimetypes
import hashlib
import json
import re
import math
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import logging

from django.conf import settings
from django.http import HttpRequest, HttpResponse
from django.shortcuts import render
from django.utils import timezone
from django.views.decorators.cache import never_cache
from django.views.decorators.http import require_GET

from ragapp.models import MyLog

log = logging.getLogger(__name__)

# ì„ íƒ: í‘œ ìŠ¤í‚¤ë§ˆ / ê²€ìƒ‰ ê·œì¹™ ëª¨ë¸ (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
try:
    from ragapp.models import TableSchema, TableSearchRule  # type: ignore
except Exception:  # pragma: no cover
    TableSchema = None  # type: ignore
    TableSearchRule = None  # type: ignore

# Vertex ì„ë² ë”©/LLM í—¬í¼
try:
    from ragapp.services.vertex_embed import (
        embed_image_file,           # ì´ë¯¸ì§€ â†’ ë²¡í„° (Vertex ë©€í‹°ëª¨ë‹¬)
        embed_text_mm,              # í…ìŠ¤íŠ¸ â†’ ë©€í‹°ëª¨ë‹¬(ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
        embed_texts_vertex as embed_texts,  # í…ìŠ¤íŠ¸ â†’ Vertex í…ìŠ¤íŠ¸ ì„ë² ë”©
        infer_table_query_with_vertex,      # (ì„ íƒ) í‘œ ì§ˆì˜ í•´ì„ìš© LLM í—¬í¼
    )
except Exception:  # pragma: no cover
    # infer_table_query_with_vertex ê°€ ì—†ê±°ë‚˜ import ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ëŠ” ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ
    from ragapp.services.vertex_embed import (
        embed_image_file,
        embed_text_mm,
        embed_texts_vertex as embed_texts,
    )
    infer_table_query_with_vertex = None  # type: ignore

# Chroma ë²¡í„° ìŠ¤í† ì–´ (ì´ë¯¸ì§€/í‘œ ëª¨ë‘ ì—¬ê¸°ì— ì €ì¥)
from ragapp.services.chroma_media import (
    add_image_item,                  # media_imagesì— add
    search_images_by_text_embedding, # í…ìŠ¤íŠ¸ë²¡í„°ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
    add_table_rows,                  # table_rowsì— í–‰ ì¶”ê°€
    search_table_by_text_embedding,  # í‘œ(í–‰) ê²€ìƒ‰
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ìŠ¤ìœ„ì¹˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PUBLIC_ALLOW_UPLOAD_IMAGES = (os.environ.get("PUBLIC_ALLOW_UPLOAD_IMAGES", "1").lower() not in ("0", "false", "no"))
PUBLIC_ALLOW_UPLOAD_CSV    = (os.environ.get("PUBLIC_ALLOW_UPLOAD_CSV", "1").lower() not in ("0", "false", "no"))
PUBLIC_MAX_FILES           = int(os.environ.get("PUBLIC_MAX_FILES", "10"))
PUBLIC_MAX_FILE_MB         = int(os.environ.get("PUBLIC_MAX_FILE_MB", "15"))
PUBLIC_MAX_CSV_ROWS        = int(os.environ.get("PUBLIC_MAX_CSV_ROWS", "1000"))

CHROMA_MEDIA_DIR           = os.environ.get("CHROMA_MEDIA_DIR", "chroma_media")

# âœ… settings.py ì—ì„œ MEDIA_ROOT=/.../uploads, MEDIA_URL=/uploads/ ë¥¼ ì“°ëŠ” ê±¸ ê¶Œì¥
MEDIA_ROOT = Path(getattr(settings, "MEDIA_ROOT", Path(settings.BASE_DIR) / "uploads")).resolve()
MEDIA_URL  = getattr(settings, "MEDIA_URL", "/uploads/")
MEDIA_ROOT.mkdir(parents=True, exist_ok=True)

# í‘œ ì›ë³¸ ë°ì´í„°ë¥¼ JSON ìœ¼ë¡œ ë³´ê´€í•  ë””ë ‰í„°ë¦¬
TABLE_DATA_DIR = MEDIA_ROOT / "table_data"
TABLE_DATA_DIR.mkdir(parents=True, exist_ok=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _log(request: HttpRequest, mode: str, query: str, ok: bool, extra: Dict[str, Any]):
    """ê°„ë‹¨ ì„œë²„ ë¡œê·¸ + MyLog í…Œì´ë¸”ì— ë‚¨ê¸°ê¸°."""
    try:
        ip = (
            request.META.get("REMOTE_ADDR", "")
            or request.META.get("HTTP_X_FORWARDED_FOR", "")
            or ""
        )
        MyLog.objects.create(
            mode_text=mode[:100],
            query=query[:500],
            ok_flag=ok,
            remote_addr_text=ip[:200],
            extra_json=extra,
        )
    except Exception:
        # ë¡œê¹… ì‹¤íŒ¨ëŠ” ì„œë¹„ìŠ¤ì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡ ë¬´ì‹œ
        pass


def _safe_media_url(abs_path: str) -> Optional[str]:
    """
    MEDIA_ROOT ë‚´ë¶€ì˜ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ -> ë¸Œë¼ìš°ì €ìš© MEDIA_URL ë¡œ ë³€í™˜
    íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ None
    """
    try:
        p = Path(abs_path).resolve()
        if not p.exists():
            return None
        if str(p).startswith(str(MEDIA_ROOT)):
            rel = p.relative_to(MEDIA_ROOT).as_posix()
            return MEDIA_URL.rstrip("/") + "/" + rel.lstrip("/")
    except Exception:
        pass
    return None


def _int(v, default):
    try:
        return int(v)
    except Exception:
        return default


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (1) ì´ë¯¸ì§€ ì¸ë±ì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@never_cache
def media_index_view(request: HttpRequest) -> HttpResponse:
    """
    ì´ë¯¸ì§€ ì—…ë¡œë“œ â†’ Vertex ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© â†’ Chroma(media_images) ì €ì¥.
    """
    if request.method == "GET":
        return render(
            request,
            "ragapp/media_index.html",
            {
                "allow_upload": PUBLIC_ALLOW_UPLOAD_IMAGES,
                "max_files": PUBLIC_MAX_FILES,
                "max_file_mb": PUBLIC_MAX_FILE_MB,
            },
        )

    # POST (ì—…ë¡œë“œ)
    if not PUBLIC_ALLOW_UPLOAD_IMAGES:
        return render(
            request,
            "ragapp/media_index.html",
            {
                "allow_upload": False,
                "error": "ì´ë¯¸ì§€ ì—…ë¡œë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.",
            },
        )

    files = request.FILES.getlist("images")[:PUBLIC_MAX_FILES]
    use_caption = bool(request.POST.get("caption_from_name"))

    # âœ… MEDIA_ROOT/images/YYYY/MM í˜•íƒœë¡œ ì €ì¥ (ì¤‘ë³µ 'uploads' ë°©ì§€)
    root = MEDIA_ROOT / "images" / timezone.now().strftime("%Y/%m")
    root.mkdir(parents=True, exist_ok=True)

    cards: List[Dict[str, Any]] = []
    ok, fail = 0, 0

    for f in files:
        status = "OK"
        msg = ""
        pid = "-"
        url = "-"
        mime = "-"
        sha16 = "-"
        try:
            if (f.size or 0) > PUBLIC_MAX_FILE_MB * 1024 * 1024:
                raise RuntimeError(f"{PUBLIC_MAX_FILE_MB}MB ì œí•œ ì´ˆê³¼")
            safe_name = os.path.basename(f.name)
            ts = timezone.now().strftime("%Y%m%d%H%M%S%f")
            dst = root / f"{ts}_{safe_name}"
            with open(dst, "wb") as out:
                for chunk in f.chunks():
                    out.write(chunk)
            mime = mimetypes.guess_type(str(dst))[0] or "application/octet-stream"

            # ğŸ”¹ Vertex MultiModalEmbeddingModel ë¡œ ì´ë¯¸ì§€ ì„ë² ë”©
            vec = embed_image_file(str(dst), mime=mime)
            pid = add_image_item(
                path=str(dst),
                embedding=vec,
                caption=(dst.stem if use_caption else ""),
            )

            # SHA-256 ì¶•ì•½ê°’ (ì¤‘ë³µ ì²´í¬/ë””ë²„ê¹…ìš©)
            h = hashlib.sha256()
            with open(dst, "rb") as rf:
                for c in iter(lambda: rf.read(8192), b""):
                    h.update(c)
            sha16 = h.hexdigest()[:16]

            # âœ… ì¡´ì¬ íŒŒì¼ë§Œ URL ìƒì„±
            url = _safe_media_url(str(dst)) or "(ë¹„ê³µê°œ ê²½ë¡œ)"
            ok += 1
        except Exception as e:
            status = "FAIL"
            msg = str(e)
            fail += 1

        cards.append(
            {
                "status": status,
                "msg": msg,
                "pid": pid,
                "url": url,
                "mime": mime,
                "sha16": sha16,
            }
        )

    _log(
        request,
        "media_index",
        f"{len(files)} files",
        True,
        {"ok": ok, "fail": fail},
    )
    return render(
        request,
        "ragapp/media_index.html",
        {
            "allow_upload": PUBLIC_ALLOW_UPLOAD_IMAGES,
            "max_files": PUBLIC_MAX_FILES,
            "max_file_mb": PUBLIC_MAX_FILE_MB,
            "cards": cards,
            "ok": ok,
            "fail": fail,
        },
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (2) í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ê²€ìƒ‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@never_cache
def media_search_view(request: HttpRequest) -> HttpResponse:
    """
    í…ìŠ¤íŠ¸(ì„¤ëª…) â†’ Vertex ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ ì„ë² ë”© â†’ Chroma(media_images) ê²€ìƒ‰.
    """
    q = (request.GET.get("q") or "").strip()
    size = max(1, min(_int(request.GET.get("size"), 12), 48))
    page = max(1, _int(request.GET.get("page"), 1))
    k = max(1, min(_int(request.GET.get("k"), 120), 600))
    hits: List[Dict[str, Any]] = []
    total_considered = 0

    if q:
        try:
            qv = embed_text_mm(q)
            top_n = min(page * size, k)  # ìš”ì²­ í˜ì´ì§€ê¹Œì§€ í™•ë³´
            res = search_images_by_text_embedding(
                text_embedding=qv, k=top_n
            ) or {}
            ids = (res.get("ids") or [[]])[0]
            metas = (res.get("metadatas") or [[]])[0]
            docs = (res.get("documents") or [[]])[0]
            total_considered = len(ids)

            start = (page - 1) * size
            end = min(start + size, len(ids))
            for pid, meta, doc in zip(
                ids[start:end],
                metas[start:end],
                docs[start:end],
            ):
                path = (meta or {}).get("path", "") or (meta or {}).get(
                    "filepath", ""
                )
                url = (meta or {}).get("url") or ""
                if not url and path:
                    url = _safe_media_url(path) or ""

                hits.append(
                    {
                        "pid": pid,
                        "caption": (
                            doc
                            or (meta or {}).get("caption")
                            or "(ìº¡ì…˜ ì—†ìŒ)"
                        ),
                        "path": path,
                        "url": url,
                    }
                )
        except Exception as e:
            return render(
                request,
                "ragapp/media_search.html",
                {
                    "q": q,
                    "size": size,
                    "page": page,
                    "k": k,
                    "error": str(e),
                },
            )

    has_prev = page > 1
    has_next = (page * size) < min(k, total_considered)
    return render(
        request,
        "ragapp/media_search.html",
        {
            "q": q,
            "size": size,
            "page": page,
            "k": k,
            "hits": hits,
            "has_prev": has_prev,
            "has_next": has_next,
        },
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (3) í‘œ ì¸ë±ì‹± (CSV + ì—‘ì…€)
#   - ì—…ë¡œë“œí•œ í‘œë¥¼ JSON(ì›ë³¸) + Vertex ì„ë² ë”© + Chroma(table_rows)ì— í•¨ê»˜ ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@never_cache
def table_index_view(request: HttpRequest) -> HttpResponse:
    if request.method == "GET":
        return render(
            request,
            "ragapp/table_index.html",
            {
                "allow_upload": PUBLIC_ALLOW_UPLOAD_CSV,
                "max_rows": PUBLIC_MAX_CSV_ROWS,
            },
        )

    if not PUBLIC_ALLOW_UPLOAD_CSV:
        return render(
            request,
            "ragapp/table_index.html",
            {
                "allow_upload": False,
                "error": "í‘œ ì—…ë¡œë“œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
            },
        )

    table_name = (request.POST.get("table_name") or "").strip()
    f = request.FILES.get("csvfile")
    limit = _int(request.POST.get("limit"), 0)

    if not table_name or not f:
        return render(
            request,
            "ragapp/table_index.html",
            {
                "allow_upload": PUBLIC_ALLOW_UPLOAD_CSV,
                "max_rows": PUBLIC_MAX_CSV_ROWS,
                "error": "í‘œ ì´ë¦„ê³¼ íŒŒì¼ì„ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
            },
        )

    # âœ… MEDIA_ROOT/tables/YYYY/MM ì— ì €ì¥
    root = MEDIA_ROOT / "tables" / timezone.now().strftime("%Y/%m")
    root.mkdir(parents=True, exist_ok=True)
    safe_name = os.path.basename(f.name)
    dst = root / f"{timezone.now().strftime('%Y%m%d%H%M%S%f')}_{safe_name}"

    rows: List[Dict[str, Any]] = []

    try:
        # 1) íŒŒì¼ ì €ì¥
        with open(dst, "wb") as out:
            for chunk in f.chunks():
                out.write(chunk)

        # 2) í™•ì¥ìì— ë”°ë¼ CSV / ì—‘ì…€ ë¶„ê¸°
        ext = dst.suffix.lower()

        if ext in [".xlsx", ".xls"]:
            # ì—‘ì…€ ì²˜ë¦¬
            try:
                import openpyxl  # í•„ìš”í•˜ë©´: pip install openpyxl
            except ImportError:
                raise RuntimeError(
                    "ì—‘ì…€ íŒŒì¼(.xlsx, .xls)ì„ ì½ìœ¼ë ¤ë©´ 'openpyxl' íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                    "í„°ë¯¸ë„ì—ì„œ 'pip install openpyxl' ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                )

            wb = openpyxl.load_workbook(dst, data_only=True)
            sheet = wb.active

            # ì²« í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
            header = None
            for row in sheet.iter_rows(min_row=1, max_row=1, values_only=True):
                header = [
                    str(c).strip() if c is not None else "" for c in row
                ]
                break

            if not header or all(not h for h in header):
                raise RuntimeError(
                    "ì—‘ì…€ íŒŒì¼ì—ì„œ ì—´ ì´ë¦„(ì²« ì¤„)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            max_rows = PUBLIC_MAX_CSV_ROWS
            limit_rows = min(limit or max_rows, max_rows)

            for idx, row in enumerate(
                sheet.iter_rows(min_row=2, values_only=True),
                start=2,
            ):
                rec: Dict[str, Any] = {}
                for i, col in enumerate(header):
                    col_name = col or f"col_{i+1}"
                    val = row[i] if row and i < len(row) else None
                    rec[col_name] = val
                rows.append(rec)
                if limit_rows and len(rows) >= limit_rows:
                    break

        else:
            # CSV ì²˜ë¦¬ (ê¸°ë³¸ê°’)
            with open(dst, "r", encoding="utf-8", newline="") as rf:
                reader = csv.DictReader(rf)
                max_rows = PUBLIC_MAX_CSV_ROWS
                limit_rows = min(limit or max_rows, max_rows)
                for row in reader:
                    rows.append(row)
                    if limit_rows and len(rows) >= limit_rows:
                        break

        # âš ï¸ í–‰ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë°”ë¡œ ë¦¬í„´
        if not rows:
            return render(
                request,
                "ragapp/table_index.html",
                {
                    "allow_upload": PUBLIC_ALLOW_UPLOAD_CSV,
                    "max_rows": PUBLIC_MAX_CSV_ROWS,
                    "error": "í‘œ ì•ˆì— ì½ì„ ìˆ˜ ìˆëŠ” ì¤„ì´ ì—†ìŠµë‹ˆë‹¤.",
                },
            )

        # 3) ìŠ¤í‚¤ë§ˆ / ìƒ˜í”Œ ì •ë³´ DBì— ì €ì¥ (TableSchema)
        try:
            if TableSchema is not None:
                cols = list(rows[0].keys())
                sample_rows = rows[:5]

                from datetime import datetime

                def _infer_type(val: Any) -> str:
                    if val is None:
                        return "text"
                    s = str(val).strip()
                    if not s:
                        return "text"
                    # ìˆ«ìì²˜ëŸ¼ ë³´ì´ë©´ number
                    try:
                        float(str(s).replace(",", ""))
                        return "number"
                    except Exception:
                        pass
                    # ISO í˜•ì‹ ë‚ ì§œ/ì‹œê°„ ì¶”ì •
                    try:
                        datetime.fromisoformat(s)
                        return "date"
                    except Exception:
                        return "text"

                column_types: Dict[str, str] = {}
                for col in cols:
                    inferred = "text"
                    for r in sample_rows:
                        v = r.get(col)
                        if v not in (None, ""):
                            inferred = _infer_type(v)
                            break
                    column_types[col] = inferred

                TableSchema.objects.update_or_create(
                    table_name=table_name,
                    defaults={
                        "columns": cols,
                        "column_types": column_types,
                        "sample_rows": sample_rows,
                    },
                )
        except Exception:
            # ìŠ¤í‚¤ë§ˆ ì €ì¥ì— ì‹¤íŒ¨í•´ë„ ì—…ë¡œë“œ/ì¸ë±ì‹±ì€ ê³„ì† ê°„ë‹¤
            pass

        # 4) ì›ë³¸ í–‰ ì „ì²´ë¥¼ JSON íŒŒì¼ë¡œë„ ë³´ê´€ (LLM í•„í„° + í‚¤ì›Œë“œ fallbackìš©)
        try:
            json_path = TABLE_DATA_DIR / f"{table_name}.json"
            with open(json_path, "w", encoding="utf-8") as jf:
                json.dump(rows, jf, ensure_ascii=False)
        except Exception:
            # JSON ì €ì¥ ì‹¤íŒ¨í•´ë„ ì¸ë±ì‹±ì€ ê³„ì†
            pass

        # 5) ë²¡í„° ì¸ë±ì‹± (Vertex text-embedding-004 + Chroma)
        def row_to_str(r: Dict[str, Any]) -> str:
            return " | ".join(f"{k}:{r.get(k,'')}" for k in r.keys())

        texts = [row_to_str(r) for r in rows]
        if not texts:
            raise RuntimeError("ì¸ë±ì‹±í•  í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")

        embs = embed_texts(texts)
        added = add_table_rows(
            table_name=table_name,
            rows=rows,
            embeddings=embs,
        )

        _log(
            request,
            "table_index",
            table_name,
            True,
            {"rows": len(rows), "added": added},
        )

        return render(
            request,
            "ragapp/table_index.html",
            {
                "allow_upload": PUBLIC_ALLOW_UPLOAD_CSV,
                "max_rows": PUBLIC_MAX_CSV_ROWS,
                "ok": True,
                "table_name": table_name,
                "added": added,
                "filename": dst.name,
            },
        )

    except Exception as e:
        rows_count = len(rows) if isinstance(rows, list) else 0
        _log(
            request,
            "table_index",
            table_name,
            False,
            {"rows": rows_count, "error": str(e)},
        )
        return render(
            request,
            "ragapp/table_index.html",
            {
                "allow_upload": PUBLIC_ALLOW_UPLOAD_CSV,
                "max_rows": PUBLIC_MAX_CSV_ROWS,
                "error": str(e),
            },
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (4) í‘œ ê²€ìƒ‰ + ê·¸ë£¹/ì§‘ê³„ (ìë™ ì¶”ë¡  + Vertex LLM/ì„ë² ë”©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ALLOWED_AGG = {"", "count", "sum", "avg", "min", "max"}

# ì§ˆë¬¸ì—ì„œ ì§‘ê³„ ì˜ë„ë¥¼ ëŒ€ì¶© ì½ì–´ë‚´ê¸° ìœ„í•œ íŒíŠ¸ (ê¸°ë³¸ê°’)
AGG_HINTS: dict[str, list[str]] = {
    "sum": ["í•©ê³„", "ì´ ", "ì „ì²´", "ì´ì•¡", "ì´ë§¤ì¶œ", "total"],
    "avg": ["í‰ê· ", "í‰ê· ì ìœ¼ë¡œ", "average", "avg"],
    "max": ["ìµœëŒ€", "ê°€ì¥ í°", "ì œì¼ í°", "ê°€ì¥ ë†’ì€", "top"],
    "min": ["ìµœì†Œ", "ê°€ì¥ ì‘ì€", "ê°€ì¥ ë‚®ì€"],
    "count": ["ê°œìˆ˜", "ê±´ìˆ˜", "ëª‡ ê°œ", "ëª‡ê°œ", "ëª‡ ëª…", "ëª‡ëª…", "row ìˆ˜"],
}

# ì»¬ëŸ¼ ì˜ë¯¸ì— ëŒ€í•œ í•œêµ­ì–´/ì˜ì–´ ë³„ì¹­ (ê¸°ë³¸ê°’)
COLUMN_SYNONYMS: dict[str, list[str]] = {
    "region": ["ì§€ì—­", "ì§€ì—­ë³„", "ë„ì‹œ", "ì‹œë„", "branch", "ì§€ì "],
    "product": ["ìƒí’ˆ", "ë©”ë‰´", "ì œí’ˆ", "ë©”ë‰´ëª…", "item"],
    "channel": ["ì±„ë„", "íŒë§¤ì±„ë„", "íŒë§¤ ê²½ë¡œ", "sales channel"],
    "date": ["ë‚ ì§œ", "ì¼ì", "date", "ì¼ë³„"],
    "sales": ["ë§¤ì¶œ", "ë§¤ì¶œì•¡", "ê¸ˆì•¡", "íŒë§¤ê¸ˆì•¡", "revenue", "sales"],
}

NUMERIC_HINTS = ["sales", "amount", "revenue", "price", "qty", "quantity", "count"]


def _safe_json_dict(text: Any) -> Dict[str, Any]:
    if not text:
        return {}
    try:
        data = json.loads(text)
        if isinstance(data, dict):
            return data
    except Exception:
        pass
    return {}


def _safe_json_list(text: Any) -> List[Any]:
    if not text:
        return []
    try:
        data = json.loads(text)
        if isinstance(data, list):
            return data
    except Exception:
        pass
    return []


def _load_table_search_config(
    table: str,
) -> Tuple[Dict[str, List[str]], Dict[str, List[str]], List[str], float, bool]:
    """
    TableSearchRule ì—ì„œ í˜„ì¬ ê²€ìƒ‰ì— ì“¸ ê·œì¹™ì„ ë¡œë“œ.
    - table ì— ë§ëŠ” í™œì„± ê·œì¹™ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    - ì—†ìœ¼ë©´ table_name ì´ ë¹„ì–´ìˆëŠ” 'ê³µí†µ ê·œì¹™'ì„ ì‚¬ìš©
    - ê²°êµ­ ëª» ì°¾ìœ¼ë©´ ì½”ë“œ ê¸°ë³¸ê°’ìœ¼ë¡œ ë°˜í™˜
    """
    agg_hints = {k: list(v) for k, v in AGG_HINTS.items()}
    column_synonyms = {k: list(v) for k, v in COLUMN_SYNONYMS.items()}
    numeric_hints = list(NUMERIC_HINTS)
    min_sim = 0.35
    hard_filter_enabled = True

    if TableSearchRule is None:
        return agg_hints, column_synonyms, numeric_hints, min_sim, hard_filter_enabled

    try:
        qs = TableSearchRule.objects.filter(is_active=True)
        rule = None
        if table:
            rule = (
                qs.filter(table_name=table)
                .order_by("-updated_at", "-id")
                .first()
            )
        if rule is None:
            rule = (
                qs.filter(table_name__in=["", None])
                .order_by("-updated_at", "-id")
                .first()
            )
    except Exception:
        rule = None

    if not rule:
        return agg_hints, column_synonyms, numeric_hints, min_sim, hard_filter_enabled

    # min_sim
    try:
        if getattr(rule, "min_sim", None) is not None:
            min_sim = float(rule.min_sim)
    except Exception:
        pass

    hard_filter_enabled = bool(getattr(rule, "hard_filter_enabled", True))

    # agg_hints_json: {"sum":["í•©ê³„","ì´ì•¡"], ...}
    override_agg = _safe_json_dict(getattr(rule, "agg_hints_json", None))
    for key, words in override_agg.items():
        if isinstance(words, list):
            agg_hints[str(key)] = [str(w) for w in words]
        elif isinstance(words, str):
            agg_hints[str(key)] = [words]

    # column_synonyms_json: {"region":["ì§€ì—­","ì§€ì "], ...}
    override_syn = _safe_json_dict(getattr(rule, "column_synonyms_json", None))
    for key, syns in override_syn.items():
        if isinstance(syns, list):
            column_synonyms[str(key)] = [str(s) for s in syns]
        elif isinstance(syns, str):
            column_synonyms[str(key)] = [syns]

    # numeric_hints_json: ["sales","amount", ...]
    override_num = _safe_json_list(getattr(rule, "numeric_hints_json", None))
    if override_num:
        numeric_hints = [str(x) for x in override_num]

    return agg_hints, column_synonyms, numeric_hints, min_sim, hard_filter_enabled


def _get_table_schema_info(table_name: str) -> tuple[list[str], dict[str, str]]:
    """
    TableSchema ì—ì„œ ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ì™€ column_types(dict)ë¥¼ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ êº¼ëƒ„.
    """
    if TableSchema is None or not table_name:
        return [], {}

    try:
        schema = (
            TableSchema.objects.filter(table_name=table_name)
            .order_by("-updated_at", "-created_at", "-id")
            .first()
        )
    except Exception:
        schema = None

    if schema is None:
        return [], {}

    cols_raw = getattr(schema, "columns", None)
    col_types_raw = getattr(schema, "column_types", None) or getattr(
        schema, "column_types_json", None
    )

    # columns íŒŒì‹±
    cols: list[str] = []
    if isinstance(cols_raw, list):
        for item in cols_raw:
            if isinstance(item, dict):
                name = item.get("name") or item.get("column") or item.get("key")
            else:
                name = str(item)
            if name and name not in cols:
                cols.append(name)
    elif isinstance(cols_raw, dict):
        cols = list(cols_raw.keys())
    elif isinstance(cols_raw, str):
        try:
            j = json.loads(cols_raw)
            if isinstance(j, list):
                for item in j:
                    if isinstance(item, dict):
                        name = item.get("name") or item.get("column") or item.get("key")
                    else:
                        name = str(item)
                    if name and name not in cols:
                        cols.append(name)
            elif isinstance(j, dict):
                cols = list(j.keys())
        except Exception:
            cols = [c.strip() for c in cols_raw.split(",") if c.strip()]

    # column_types íŒŒì‹±
    col_types: dict[str, str] = {}
    if isinstance(col_types_raw, dict):
        col_types = {str(k): str(v) for k, v in col_types_raw.items()}
    elif isinstance(col_types_raw, str):
        try:
            j = json.loads(col_types_raw)
            if isinstance(j, dict):
                col_types = {str(k): str(v) for k, v in j.items()}
        except Exception:
            pass

    return cols, col_types


def _guess_agg_from_question(q: str, agg_hints: Dict[str, List[str]]) -> str:
    q_lower = q.lower()
    for agg_key, words in agg_hints.items():
        for w in words:
            if w in q or w in q_lower:
                return agg_key
    return ""


def _auto_fill_table_and_agg(
    q: str,
    table: str,
    group_by: str,
    agg_field: str,
    agg: str,
    agg_hints: Dict[str, List[str]],
    column_synonyms: Dict[str, List[str]],
    numeric_hints: List[str],
) -> tuple[str, str, str, str]:
    """
    ì‚¬ìš©ìê°€ ê³ ê¸‰ ì„¤ì •ì„ ë¹„ì› ì„ ë•Œ, TableSchema + ì§ˆë¬¸ì„ ë³´ê³ 
    table / group_by / agg_field / agg ë¥¼ ìµœëŒ€í•œ ìë™ìœ¼ë¡œ ì±„ì›Œì¤Œ.
    ì´ë¯¸ ì‚¬ìš©ìê°€ ì ì€ ê°’ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ.
    """
    # 0) table ë¹„ì–´ ìˆê³ , TableSchema ì— í‘œê°€ ë”± 1ê°œë©´ ìë™ ì„ íƒ
    if (not table) and TableSchema is not None:
        try:
            distinct_tables = list(
                TableSchema.objects.order_by("table_name")
                .values_list("table_name", flat=True)
                .distinct()
            )
        except Exception:
            distinct_tables = []
        if len(distinct_tables) == 1:
            table = distinct_tables[0]

    # TableSchema ì—†ìœ¼ë©´ ì—¬ê¸°ê¹Œì§€
    if TableSchema is None or not table:
        return table, group_by, agg_field, agg

    cols, col_types = _get_table_schema_info(table)
    if not cols:
        return table, group_by, agg_field, agg

    # 1) agg ë¹„ì–´ ìˆìœ¼ë©´ ì§ˆë¬¸ì—ì„œ ì¶”ì •
    if not agg:
        agg = _guess_agg_from_question(q, agg_hints)

    # 2) agg_field ë¹„ì–´ ìˆìœ¼ë©´ ìˆ«ì ì»¬ëŸ¼ ì¤‘ì—ì„œ ì¶”ì •
    if not agg_field and agg:
        numeric_cols = [
            c
            for c in cols
            if col_types.get(c) in ("number", "numeric", "float", "int")
        ]
        chosen = ""
        q_lower = q.lower()

        for c in numeric_cols:
            name_lower = c.lower()
            if any(h in name_lower for h in numeric_hints):
                chosen = c
                break
            if name_lower in q_lower:
                chosen = c
                break

        if not chosen and len(numeric_cols) == 1:
            chosen = numeric_cols[0]

        if chosen:
            agg_field = chosen

    # 3) group_by ë¹„ì–´ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸/ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì—ì„œ ì¶”ì •
    if not group_by and agg and agg_field:
        q_lower = q.lower()
        text_cols = [
            c
            for c in cols
            if col_types.get(c, "text") not in ("number", "numeric", "float", "int")
        ]

        candidate_scores: list[tuple[int, str]] = []

        for c in text_cols:
            score = 0
            name_lower = c.lower()

            if name_lower in q_lower:
                score += 5

            for key, syns in column_synonyms.items():
                if key in name_lower:
                    if any(s in q for s in syns):
                        score += 4
                    else:
                        score += 1

            if score > 0:
                candidate_scores.append((score, c))

        if candidate_scores:
            candidate_scores.sort(reverse=True)
            group_by = candidate_scores[0][1]
        else:
            if len(text_cols) == 1:
                group_by = text_cols[0]

    return table, group_by, agg_field, agg


def _infer_columns(hit_rows: List[Dict[str, Any]], table: Optional[str]) -> List[str]:
    """
    ê²€ìƒ‰ëœ í–‰ë“¤ì—ì„œ ì»¬ëŸ¼ ì´ë¦„ì„ ì¶”ë ¤ì„œ ë³´ì—¬ì¤„ ìˆœì„œë¥¼ ì •í•¨.
    - table ì´ None ì´ë©´ ì—¬ëŸ¬ í‘œê°€ ì„ì¸ ìƒí™© â†’ _table ì»¬ëŸ¼ì„ ë§¨ ì•ìœ¼ë¡œ ë³´ëƒ„.
    """
    if not hit_rows:
        return []

    seen: List[str] = []
    for r in hit_rows:
        if not isinstance(r, dict):
            continue
        for k in r.keys():
            if k not in seen:
                seen.append(k)

    if not table and "_table" in seen:
        seen.remove("_table")
        seen.insert(0, "_table")

    return seen


def _to_float(v: Any) -> Optional[float]:
    """ì§‘ê³„ìš© ìˆ«ì ë³€í™˜ (ë¬¸ìì—´/ì½¤ë§ˆ í¬í•¨ë„ ì²˜ë¦¬). ì‹¤íŒ¨í•˜ë©´ None."""
    try:
        if v is None:
            return None
        if isinstance(v, (int, float)):
            return float(v)
        s = str(v).replace(",", "").strip()
        if not s:
            return None
        return float(s)
    except Exception:
        return None


def _apply_group_agg(
    hit_rows: List[Dict[str, Any]],
    group_by: str,
    agg: str,
    agg_field: str,
) -> Tuple[List[Dict[str, Any]], List[str]]:
    """
    group_by / agg / agg_field ì„¤ì •ì— ë”°ë¼ ê·¸ë£¹ë³„ count/sum/avg/min/max ê³„ì‚°.
    """
    groups: Dict[str, List[float]] = {}
    counts: Dict[str, int] = {}

    for r in hit_rows:
        if not isinstance(r, dict):
            continue

        key = str(r.get(group_by, "") or "(ê°’ ì—†ìŒ)")
        counts[key] = counts.get(key, 0) + 1

        if agg == "count" or not agg_field:
            continue

        val = _to_float(r.get(agg_field))
        if val is None:
            continue
        groups.setdefault(key, []).append(val)

    rows_out: List[Dict[str, Any]] = []

    if agg == "count" or not agg_field:
        columns = [group_by, "rows"]
        for key, count in counts.items():
            rows_out.append({group_by: key, "rows": count})
    else:
        col_name = f"{agg}_{agg_field}"
        columns = [group_by, "rows", col_name]
        for key, nums in groups.items():
            if not nums:
                continue

            if agg == "sum":
                value = sum(nums)
            elif agg == "avg":
                value = sum(nums) / len(nums)
            elif agg == "min":
                value = min(nums)
            elif agg == "max":
                value = max(nums)
            else:
                value = len(nums)

            rows_out.append(
                {
                    group_by: key,
                    "rows": counts.get(key, len(nums)),
                    col_name: value,
                }
            )

    rows_out.sort(key=lambda r: str(r.get(group_by, "")))
    return rows_out, columns


def _load_table_rows_from_file(table: str) -> List[Dict[str, Any]]:
    """
    TABLE_DATA_DIR/table_name.json ì—ì„œ ì›ë³¸ í–‰ ì „ì²´ë¥¼ ë¡œë“œ.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ ê¹¨ì ¸ ìˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸.
    """
    try:
        if not table:
            return []
        path = TABLE_DATA_DIR / f"{table}.json"
        if not path.exists():
            return []
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            return [r for r in data if isinstance(r, dict)]
        return []
    except Exception:
        return []


def _apply_filters(
    rows: List[Dict[str, Any]],
    filters: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    LLM ì´ ë§Œë“¤ì–´ì¤€ filters ë¥¼ ê·¸ëŒ€ë¡œ ì ìš©.
    í•„í„° í˜•ì‹ ì˜ˆ:
      {"column": "region", "op": "=", "value": "ì„œìš¸"}
      {"column": "product", "op": "in", "value": ["ì•„ë©”ë¦¬ì¹´ë…¸", "ë¼ë–¼"]}
      {"column": "sales", "op": ">", "value": 1000000}
    """
    if not filters:
        return rows

    def _match(row: Dict[str, Any], flt: Dict[str, Any]) -> bool:
        col = flt.get("column") or flt.get("field")
        if not col:
            return True
        op = (flt.get("op") or "=").lower()
        val = flt.get("value")
        cell = row.get(col)
        if cell is None:
            return False

        s = str(cell)
        if op in ("=", "eq"):
            if isinstance(val, list):
                return s in [str(v) for v in val]
            return s == str(val)
        if op in ("contains", "like"):
            return str(val) in s
        if op == "in":
            if not isinstance(val, list):
                return False
            return s in [str(v) for v in val]

        # ìˆ«ì ë¹„êµ (>, >=, <, <=)
        try:
            cnum = float(str(cell).replace(",", ""))
            vnum = float(str(val).replace(",", ""))
        except Exception:
            return False

        if op in (">", "gt"):
            return cnum > vnum
        if op in (">=", "ge"):
            return cnum >= vnum
        if op in ("<", "lt"):
            return cnum < vnum
        if op in ("<=", "le"):
            return cnum <= vnum

        return True

    out: List[Dict[str, Any]] = []
    for r in rows:
        ok = True
        for f in filters:
            if not _match(r, f):
                ok = False
                break
        if ok:
            out.append(r)
    return out


def _hard_filter_rows_by_question(
    q: str,
    rows: List[Dict[str, Any]],
    columns: List[str],
) -> List[Dict[str, Any]]:
    """
    ì§ˆë¬¸(q)ì— ë“¤ì–´ìˆëŠ” ë‹¨ì–´ê°€ ì‹¤ì œ í–‰ì˜ ê°’ê³¼ ì •í™•íˆ/ë¶€ë¶„ì ìœ¼ë¡œ ê²¹ì¹˜ë©´
    ê·¸ ê°’ì„ ê°€ì§„ í–‰ë§Œ ìš°ì„ ì ìœ¼ë¡œ ë‚¨ê¸°ëŠ” í•˜ë“œ í•„í„°.

    - ì˜ˆ: ì§ˆë¬¸ì— "ì„œìš¸", "ì•„ë©”ë¦¬ì¹´ë…¸"ê°€ ë“¤ì–´ìˆê³ 
      region / product ì»¬ëŸ¼ì— ê·¸ëŸ° ê°’ì´ ìˆìœ¼ë©´
      í•´ë‹¹ ê°’ì´ ë“¤ì–´ê°„ í–‰ë§Œ ìš°ì„ ì ìœ¼ë¡œ ë‚¨ê¹€.
    - í•„í„° ê²°ê³¼ê°€ 0ê±´ì´ë©´, ì›ë˜ rowsë¥¼ ê·¸ëŒ€ë¡œ ëŒë ¤ì¤˜ì„œ 'ì „ë¶€ ë‚ ì•„ê°€ëŠ”' ì¼ì€ ë§‰ëŠ”ë‹¤.
    """
    if not q or not rows or not columns:
        return rows

    q_norm = q.replace(" ", "")
    candidates = rows

    for col in columns:
        if col == "_table":
            continue

        values = sorted(
            {
                str(r.get(col, "")).strip()
                for r in rows
                if r.get(col) not in (None, "")
            }
        )
        if not values:
            continue

        hit_vals = []
        for v in values:
            v_norm = v.replace(" ", "")
            if not v_norm:
                continue
            if v in q or v_norm in q_norm:
                hit_vals.append(v)

        if hit_vals:
            hit_set = set(hit_vals)
            new_candidates = [
                r for r in candidates if str(r.get(col, "")).strip() in hit_set
            ]
            if new_candidates:
                candidates = new_candidates

    return candidates or rows


@require_GET
def table_search_view(request: HttpRequest) -> HttpResponse:
    """
    ì—…ë¡œë“œí•´ ë‘” í‘œ(table_rows)ì—ì„œ ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ í–‰/ì§‘ê³„ ê²°ê³¼ë¥¼ ì°¾ëŠ” ë·°.
    - q: ì§ˆë¬¸/í‚¤ì›Œë“œ
    - table: í…Œì´ë¸” ì´ë¦„(ì„ íƒ)
    - group_by / agg_field / agg: ê·¸ë£¹Â·ì§‘ê³„ ì˜µì…˜(ì„ íƒ, ë¹„ì›Œë‘ë©´ ìë™ ì¶”ë¡ )
      â†’ ìë™ ì¶”ë¡ ì€ TableSchema(ì»¬ëŸ¼ ì´ë¦„/íƒ€ì…) + (ì„ íƒì ìœ¼ë¡œ) Vertex LLM ê²°ê³¼ë¥¼ ì‚¬ìš©.
    """
    q = (request.GET.get("q") or "").strip()

    def _to_int_param(name: str, default: int) -> int:
        try:
            v = int(request.GET.get(name, default))
            return max(1, v)
        except Exception:
            return default

    size = _to_int_param("size", 12)
    page = _to_int_param("page", 1)
    try:
        k = int(request.GET.get("k", 200))
    except Exception:
        k = 200

    table = (request.GET.get("table") or "").strip()
    group_by = (request.GET.get("group_by") or "").strip()
    agg_field = (request.GET.get("agg_field") or "").strip()
    agg = (request.GET.get("agg") or "").strip().lower()
    if agg not in ALLOWED_AGG:
        agg = ""

    # ğŸ”¹ ê²€ìƒ‰ í¼ datalistìš© í…Œì´ë¸” ì´ë¦„ ëª©ë¡
    table_names: list[str] = []
    if TableSchema is not None:
        try:
            table_names = list(
                TableSchema.objects.order_by("table_name")
                .values_list("table_name", flat=True)
                .distinct()
            )
        except Exception:
            table_names = []

    # ğŸ”¹ TableSearchRule ì—ì„œ ê·œì¹™ê°’ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    agg_hints_cfg, column_synonyms_cfg, numeric_hints_cfg, min_sim, hard_filter_enabled = _load_table_search_config(table)

    columns: list[str] = []
    rows: list[dict] = []
    total: int = 0
    page_count: int = 1
    error_msg: str | None = None
    used_loose: bool = False  # ë„ˆë¬´ ëŠìŠ¨í•œ ê¸°ì¤€ìœ¼ë¡œ fallback í–ˆëŠ”ì§€ í‘œì‹œ

    # â— ì§ˆë¬¸ ì—†ìœ¼ë©´ í¼ë§Œ
    if not q:
        ctx = {
            "q": q,
            "size": size,
            "page": page,
            "k": k,
            "table": table,
            "group_by": group_by,
            "agg_field": agg_field,
            "agg": agg,
            "columns": columns,
            "rows": rows,
            "total": total,
            "page_count": page_count,
            "error_msg": error_msg,
            "table_names": table_names,
            "used_loose": used_loose,
        }
        return render(request, "ragapp/table_search.html", ctx)

    # 1ì°¨: TableSchema ê¸°ë°˜ ìë™ ì±„ìš°ê¸°
    orig_table = table
    table, group_by, agg_field, agg = _auto_fill_table_and_agg(
        q=q,
        table=table,
        group_by=group_by,
        agg_field=agg_field,
        agg=agg,
        agg_hints=agg_hints_cfg,
        column_synonyms=column_synonyms_cfg,
        numeric_hints=numeric_hints_cfg,
    )

    # table ì´ ìë™ìœ¼ë¡œ ì±„ì›Œì§„ ê²½ìš°, ê·¸ í…Œì´ë¸” ê¸°ì¤€ ê·œì¹™ì„ ë‹¤ì‹œ í•œ ë²ˆ ë¡œë“œ
    if not orig_table and table != orig_table:
        agg_hints_cfg, column_synonyms_cfg, numeric_hints_cfg, min_sim, hard_filter_enabled = _load_table_search_config(table)

    # 2ì°¨: (ì„ íƒ) Vertex LLM ìœ¼ë¡œ ì§ˆì˜ êµ¬ì¡° í•´ì„
    llm_plan: Optional[Dict[str, Any]] = None
    if infer_table_query_with_vertex is not None and TableSchema is not None:
        try:
            # ê° í‘œì˜ ì»¬ëŸ¼/íƒ€ì…/ìƒ˜í”Œ í–‰ì„ ëª¨ì•„ì„œ LLMì— ë„˜ê¸¸ ìš”ì•½ êµ¬ì„±
            tables_for_llm: Dict[str, Dict[str, Any]] = {}
            for tname in table_names:
                cols, col_types = _get_table_schema_info(tname)
                sample_rows: List[Dict[str, Any]] = []
                try:
                    schema_obj = (
                        TableSchema.objects.filter(table_name=tname)
                        .order_by("-updated_at", "-created_at", "-id")
                        .first()
                    )
                    if schema_obj is not None:
                        sr = getattr(schema_obj, "sample_rows", None)
                        if isinstance(sr, list):
                            sample_rows = [
                                r for r in sr if isinstance(r, dict)
                            ]
                        elif isinstance(sr, str):
                            try:
                                j = json.loads(sr)
                                if isinstance(j, list):
                                    sample_rows = [
                                        r for r in j if isinstance(r, dict)
                                    ]
                            except Exception:
                                sample_rows = []
                except Exception:
                    sample_rows = []

                tables_for_llm[tname] = {
                    "columns": cols,
                    "column_types": col_types,
                    "sample_rows": sample_rows,
                }

            llm_plan = infer_table_query_with_vertex(
                question=q,
                tables=tables_for_llm,
                default_table=table or None,
            )
        except Exception as e:
            log.exception("infer_table_query_with_vertex ì‹¤íŒ¨: %s", e)
            llm_plan = None

    # LLM ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ table / group_by / agg ì„¤ì •ì„ ë³´ê°•
    plan_filters: List[Dict[str, Any]] = []
    if isinstance(llm_plan, dict):
        plan_table = (llm_plan.get("table") or "").strip()
        if plan_table and not table:
            table = plan_table

        if not group_by and llm_plan.get("group_by"):
            group_by = str(llm_plan.get("group_by"))

        if not agg_field and llm_plan.get("agg_field"):
            agg_field = str(llm_plan.get("agg_field"))

        if not agg and llm_plan.get("agg"):
            agg_candidate = str(llm_plan.get("agg")).lower()
            if agg_candidate in ALLOWED_AGG:
                agg = agg_candidate

        pf = llm_plan.get("filters") or llm_plan.get("where") or []
        if isinstance(pf, list):
            plan_filters = [f for f in pf if isinstance(f, dict)]

    try:
        # 3) ì§ˆë¬¸ â†’ Vertex í…ìŠ¤íŠ¸ ì„ë² ë”© (text-embedding-004)
        q_vecs = embed_texts([q]) or []
        if not q_vecs:
            raise RuntimeError("ì„ë² ë”©ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        qv = q_vecs[0]

        # 4) table_rowsì—ì„œ kê°œ ê²€ìƒ‰ (Chroma)
        res = search_table_by_text_embedding(text_embedding=qv, k=k) or {}
        metas_raw = res.get("metadatas") or []
        dists_raw = res.get("distances") or []

        # Chroma ì‘ë‹µ flatten
        if isinstance(metas_raw, list) and metas_raw:
            if isinstance(metas_raw[0], list):
                metas = metas_raw[0] or []
            else:
                metas = metas_raw
        else:
            metas = []

        if isinstance(dists_raw, list) and dists_raw:
            if isinstance(dists_raw[0], list):
                dists = dists_raw[0] or []
            else:
                dists = dists_raw
        else:
            dists = []

        # ê¸¸ì´ ë§ì¶”ê¸°
        if len(dists) < len(metas):
            dists = dists + [None] * (len(metas) - len(dists))
        elif len(dists) > len(metas):
            dists = dists[: len(metas)]

        # ğŸ”¹ ì—„ê²©/ëŠìŠ¨ ëª¨ë“œ ëª¨ë‘ ì €ì¥í•´ ë‘ê³ , ë‚˜ì¤‘ì— fallback
        strict_all: list[dict] = []        # ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼ (ì „ì²´)
        strict_filtered: list[dict] = []   # ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼ + table í•„í„° ì ìš©
        loose_all: list[dict] = []         # ìœ ì‚¬ë„ ë‚®ìŒê¹Œì§€ í¬í•¨ (ì „ì²´)
        loose_filtered: list[dict] = []    # ìœ ì‚¬ë„ ë‚®ìŒê¹Œì§€ í¬í•¨ + table í•„í„° ì ìš©

        MIN_SIM = float(min_sim or 0.0)

        for meta, dist in zip(metas, dists):
            if not isinstance(meta, dict):
                continue

            meta_table = (meta.get("table") or meta.get("table_name") or "").strip()
            row_json = (
                meta.get("row_json")
                or meta.get("row")
                or meta.get("data")
                or {}
            )

            if isinstance(row_json, str):
                try:
                    row = json.loads(row_json)
                except Exception:
                    row = {}
            elif isinstance(row_json, dict):
                row = row_json
            else:
                row = {}

            if not isinstance(row, dict) or not row:
                continue

            row_with_table = dict(row)
            if meta_table:
                row_with_table["_table"] = meta_table

            # ê±°ë¦¬ â†’ ìœ ì‚¬ë„
            try:
                score = 1.0 - float(dist) if dist is not None else None
            except Exception:
                score = None

            match_table = (not table) or (not meta_table) or (meta_table == table)

            # ëŠìŠ¨ ëª¨ë“œì—ëŠ” ì¼ë‹¨ ë‹¤ ë„£ê¸°
            loose_all.append(row_with_table)
            if match_table:
                loose_filtered.append(row_with_table)

            # ì—„ê²© ëª¨ë“œëŠ” MIN_SIM ì´ìƒì¼ ë•Œë§Œ
            if (score is None) or (score >= MIN_SIM):
                strict_all.append(row_with_table)
                if match_table:
                    strict_filtered.append(row_with_table)

        # 5) ë²¡í„° ê¸°ë°˜ 1ì°¨ ì„ íƒ (ì—†ìœ¼ë©´ JSON fallback ì‹œë„)
        if strict_filtered:
            parsed_rows = strict_filtered
        elif strict_all:
            parsed_rows = strict_all
        elif loose_filtered:
            parsed_rows = loose_filtered
            used_loose = True
        else:
            parsed_rows = loose_all
            used_loose = True

        # ğŸ”» ì—¬ê¸°ì„œê¹Œì§€ë„ ì•„ë¬´ í–‰ë„ ì—†ìœ¼ë©´ â†’ JSON ì›ë³¸ ê¸°ë°˜ fallback
        if not parsed_rows:
            if table:
                all_rows = _load_table_rows_from_file(table)
                if all_rows:
                    # ê°„ë‹¨ í‚¤ì›Œë“œ fallback: ì§ˆë¬¸ì— ë‚˜ì˜¨ ë‹¨ì–´ë“¤ì´ ë“¤ì–´ê°„ í–‰ ìš°ì„ 
                    import re as _re

                    def _row_to_str(r: Dict[str, Any]) -> str:
                        return " ".join(f"{k}:{r.get(k,'')}" for k in r.keys())

                    keywords = [w for w in _re.split(r"\s+", q) if w]
                    fallback_hits: List[Dict[str, Any]] = []
                    for r in all_rows:
                        s = _row_to_str(r)
                        if all(kw in s for kw in keywords):
                            fallback_hits.append(r)

                    parsed_rows = fallback_hits or all_rows
                    used_loose = True

            # JSONì—ì„œë„ ëª» ì°¾ìœ¼ë©´ ì§„ì§œë¡œ ë°ì´í„° ì—†ìŒ
            if not parsed_rows:
                ctx = {
                    "q": q,
                    "size": size,
                    "page": page,
                    "k": k,
                    "table": table,
                    "group_by": group_by,
                    "agg_field": agg_field,
                    "agg": agg,
                    "columns": [],
                    "rows": [],
                    "total": 0,
                    "page_count": 1,
                    "error_msg": None,
                    "table_names": table_names,
                    "used_loose": used_loose,
                }
                return render(request, "ragapp/table_search.html", ctx)

        # 6) LLMì´ ì œì•ˆí•œ filters ê°€ ìˆìœ¼ë©´ í•œ ë²ˆ ë” í•„í„°ë§
        if plan_filters:
            filtered = _apply_filters(parsed_rows, plan_filters)
            if filtered:  # ì „ë¶€ ë‚ ì•„ê°€ë©´ ë„ˆë¬´ ë¹¡ì„¸ë‹ˆê¹Œ, ë‚¨ì„ ë•Œë§Œ ì±„íƒ
                parsed_rows = filtered

        # 7) ì»¬ëŸ¼ ìˆœì„œ (TableSchema ìš°ì„ , ì—†ìœ¼ë©´ ìë™ ì¶”ë¡ )
        col_order: list[str] | None = None
        if TableSchema is not None and table:
            cols, _ = _get_table_schema_info(table)
            if cols:
                col_order = cols

        if col_order is None:
            col_order = _infer_columns(parsed_rows, table or None)

        # 8) ì§ˆë¬¸ ì•ˆì˜ ê°’ìœ¼ë¡œ í•˜ë“œ í•„í„° í•œ ë²ˆ ë” (ì„œìš¸/ì•„ë©”ë¦¬ì¹´ë…¸ ë“±)
        if hard_filter_enabled:
            parsed_rows = _hard_filter_rows_by_question(q, parsed_rows, col_order or [])

        # 9) ì§‘ê³„ ëª¨ë“œì¸ì§€ íŒë‹¨
        if group_by and agg and agg_field:
            rows_all, columns = _apply_group_agg(
                parsed_rows, group_by, agg, agg_field
            )
        else:
            rows_all = parsed_rows
            columns = col_order or []

        # 10) í˜ì´ì§€ë„¤ì´ì…˜
        total = len(rows_all)
        page_count = max(1, math.ceil(total / max(1, size)))
        if page > page_count:
            page = page_count
        start = max(0, (page - 1) * size)
        end = start + size
        rows = rows_all[start:end]

        # ğŸ”» ê·¸ë˜ë„ ìµœì¢…ì ìœ¼ë¡œ ì•„ë¬´ í–‰ë„ ì—†ìœ¼ë©´, JSON ì „ì²´ì—ì„œë¼ë„ ëª‡ ì¤„ ë½‘ì•„ì£¼ê¸°
        if total == 0 and table:
            all_rows = _load_table_rows_from_file(table)
            if all_rows:
                total = len(all_rows)
                page_count = max(1, math.ceil(total / max(1, size)))
                if page > page_count:
                    page = page_count
                start = max(0, (page - 1) * size)
                end = start + size
                rows = all_rows[start:end]
                columns = _infer_columns(all_rows, table)
                used_loose = True

    except Exception as e:
        error_msg = f"{e.__class__.__name__}: {e}"

    ctx = {
        "q": q,
        "size": size,
        "page": page,
        "k": k,
        "table": table,
        "group_by": group_by,
        "agg_field": agg_field,
        "agg": agg,
        "columns": columns,
        "rows": rows,
        "total": total,
        "page_count": page_count,
        "error_msg": error_msg,
        "table_names": table_names,
        "used_loose": used_loose,
    }
    return render(request, "ragapp/table_search.html", ctx)
